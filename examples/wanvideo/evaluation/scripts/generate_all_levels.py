#!/usr/bin/env python3
"""
generate_all_levels.py

æ‰¹é‡ç”Ÿæˆæ‰€æœ‰4ä¸ªçº§åˆ«çš„è¯„ä¼°è§†é¢‘
Level 0-3 Ã— 11 Prompts Ã— 3 Seeds = 132 videos

ä¿®å¤ç‰ˆæœ¬ï¼šå…¼å®¹PyTorch 2.6+
"""

import sys
import argparse
import torch
from pathlib import Path
from tqdm import tqdm
from typing import Dict

# æ·»åŠ é¡¹ç›®è·¯å¾„
repo_root = Path(__file__).resolve().parents[4]
sys.path.append(str(repo_root))

from diffsynth.pipelines.wan_video_new import WanVideoPipeline
from diffsynth.utils import ModelConfig
from diffsynth.lora import GeneralLoRALoader
from diffsynth.models.utils import load_state_dict as load_state_dict_from_file
from diffsynth.modules.temporal_module import TemporalModule
from diffsynth.modules.latent_flow_predictor import LatentFlowPredictor

# å¯¼å…¥é…ç½®
config_dir = Path(__file__).resolve().parent.parent / "config"
sys.path.append(str(config_dir))
from prompts import EVALUATION_PROMPTS, RANDOM_SEEDS, LEVEL_CONFIGS

utils_dir = Path(__file__).resolve().parent.parent / "utils"
sys.path.append(str(utils_dir))
from video_utils import save_video_frames


class MultiLevelGenerator:
    """å››çº§å¯¹æ¯”å®éªŒçš„è§†é¢‘ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        device: str = "cuda:0",
        dtype: torch.dtype = torch.float16,
        model_id: str = "Wan-AI/Wan2.1-T2V-1.3B"
    ):
        self.device = device
        self.dtype = dtype
        self.model_id = model_id
        
        # åˆå§‹åŒ–åŸºç¡€Pipeline
        print("æ­£åœ¨åˆå§‹åŒ–åŸºç¡€Pipeline...")
        model_configs = [
            ModelConfig(model_id=model_id, origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth"),
            ModelConfig(model_id=model_id, origin_file_pattern="diffusion_pytorch_model.safetensors"),
            ModelConfig(model_id=model_id, origin_file_pattern="Wan2.1_VAE.pth"),
        ]
        self.base_pipe = WanVideoPipeline.from_pretrained(
            device=device,
            torch_dtype=dtype,
            model_configs=model_configs
        )
        
        # ä¿å­˜åŸºç¡€DiTæƒé‡çš„è·¯å¾„ï¼ˆä»å·²åŠ è½½çš„æ¨¡å‹ä¸­è·å–ï¼‰
        self.base_dit_state = None
        
        # å­˜å‚¨å„çº§åˆ«çš„ç»„ä»¶
        self.lora_loader = GeneralLoRALoader(device=device, torch_dtype=dtype)
        self.temporal_module = None
        self.flow_predictor = None
        
        print("âœ“ åŸºç¡€Pipelineåˆå§‹åŒ–å®Œæˆ")
    
    def _save_base_dit_state(self):
        """ä¿å­˜åŸºç¡€DiTçš„state_dictåˆ°å†…å­˜"""
        if self.base_dit_state is None:
            print("  ä¿å­˜åŸºç¡€DiTçŠ¶æ€åˆ°å†…å­˜...")
            self.base_dit_state = {
                k: v.clone().cpu() for k, v in self.base_pipe.dit.state_dict().items()
            }
    
    def _restore_base_dit_state(self):
        """ä»å†…å­˜æ¢å¤åŸºç¡€DiTçš„state_dict"""
        if self.base_dit_state is not None:
            print("  ä»å†…å­˜æ¢å¤åŸºç¡€DiTçŠ¶æ€...")
            # å°†state_dictç§»å›GPU
            state_dict_gpu = {
                k: v.to(self.device) for k, v in self.base_dit_state.items()
            }
            self.base_pipe.dit.load_state_dict(state_dict_gpu)
    
    def _apply_lora(self, lora_path: str, alpha: float = 0.3):
        """åº”ç”¨LoRAæƒé‡"""
        print(f"  åŠ è½½LoRA: {Path(lora_path).name}")
        lora_state = load_state_dict_from_file(
            lora_path,
            torch_dtype=self.dtype,
            device=self.device
        )
        self.lora_loader.load(self.base_pipe.dit, lora_state, alpha=alpha)
    
    def _load_dit_finetuned(self, dit_path: str):
        """åŠ è½½å¾®è°ƒåçš„DiTæƒé‡"""
        print(f"  åŠ è½½å¾®è°ƒDiT: {Path(dit_path).name}")
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ  weights_only=False
        state_dict = torch.load(dit_path, map_location=self.device, weights_only=False)
        self.base_pipe.dit.load_state_dict(state_dict)
    
    def _load_temporal_module(self, temporal_module_path: str, flow_predictor_path: str):
            """åŠ è½½Temporal Moduleå’ŒFlow Predictor"""
            print(f"  åŠ è½½Temporal Module: {Path(temporal_module_path).name}")
            print(f"  åŠ è½½Flow Predictor: {Path(flow_predictor_path).name}")

            # åˆå§‹åŒ–æ¨¡å—
            self.temporal_module = TemporalModule(
                latent_channels=16,
                style_dim=None
            ).to(self.device)

            self.flow_predictor = LatentFlowPredictor(
                in_channels=16
            ).to(self.device)

            # åŠ è½½Temporal Moduleæƒé‡
            tm_checkpoint = torch.load(temporal_module_path, map_location=self.device, weights_only=False)
            if "ema" in tm_checkpoint:
                ema_state_dict = {}
                for name, param in self.temporal_module.named_parameters():
                    if name in tm_checkpoint["ema"]:
                        ema_state_dict[name] = tm_checkpoint["ema"][name]
                self.temporal_module.load_state_dict(ema_state_dict)
            elif "temporal_module" in tm_checkpoint:
                self.temporal_module.load_state_dict(tm_checkpoint["temporal_module"])
            else:
                # å‡è®¾æ–‡ä»¶æœ¬èº«å°±æ˜¯ state_dict
                self.temporal_module.load_state_dict(tm_checkpoint)
                
            # åŠ è½½Flow Predictoræƒé‡
            fp_checkpoint = torch.load(flow_predictor_path, map_location=self.device, weights_only=False)
            if "flow_predictor" in fp_checkpoint:
                self.flow_predictor.load_state_dict(fp_checkpoint["flow_predictor"])
            else:
                # å‡è®¾æ–‡ä»¶æœ¬èº«å°±æ˜¯ state_dict
                self.flow_predictor.load_state_dict(fp_checkpoint)
            # --------------------------------------------------
            # åœ¨è¿™é‡Œæ·»åŠ ä¿®æ­£ä»£ç 
            # --------------------------------------------------
            FIXED_ALPHA = 0.3
            print(f"  !! è­¦å‘Š: æ­£åœ¨ä» checkpoint åŠ è½½å, å¼ºåˆ¶è¦†ç›– alpha = {FIXED_ALPHA} !!")
            # è°ƒç”¨æ¨¡å—å†…éƒ¨çš„æ–¹æ³•æ¥è®¡ç®— logit
            init_logit = self.temporal_module._inv_sigmoid(FIXED_ALPHA)
            # ä½¿ç”¨ .data.fill_() æ¥å°±åœ°ä¿®æ”¹ buffer çš„å€¼
            self.temporal_module.alpha_param.data.fill_(init_logit)
            # --------------------------------------------------


            self.temporal_module.eval()
            self.flow_predictor.eval()
        
    def generate_single_video(
        self,
        prompt: str,
        negative_prompt: str,
        num_frames: int,
        seed: int,
        height: int = 256,
        width: int = 256,
        use_temporal: bool = False
    ) -> torch.Tensor:
        """
        ç”Ÿæˆå•ä¸ªè§†é¢‘
        
        Returns:
            torch.Tensor: (T, C, H, W) æ ¼å¼çš„è§†é¢‘å¸§
        """
        with torch.no_grad():
            if not use_temporal:
                # æ ‡å‡†ç”Ÿæˆ
                output = self.base_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    num_frames=num_frames,
                    height=height,
                    width=width,
                    seed=seed
                )
                
                # PIL Imageåˆ—è¡¨ -> Tensor
                from PIL import Image
                import numpy as np
                
                if isinstance(output, list) and len(output) > 0 and isinstance(output[0], Image.Image):
                    frames_list = []
                    for img in output:
                        img_np = np.array(img)
                        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).float() / 255.0
                        frames_list.append(img_tensor)
                    frames = torch.stack(frames_list, dim=0)
                else:
                    raise ValueError(f"æœªçŸ¥çš„è¾“å‡ºæ ¼å¼: {type(output)}")
                
                return frames
            
            else:
                # ä½¿ç”¨Temporal Moduleç”Ÿæˆ
                # å…ˆæ ‡å‡†ç”Ÿæˆ
                output = self.base_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    num_frames=num_frames,
                    height=height,
                    width=width,
                    seed=seed
                )
                
                # è½¬æ¢ä¸ºTensor
                from PIL import Image
                import numpy as np
                
                frames_list = []
                for img in output:
                    img_np = np.array(img)
                    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).float() / 255.0
                    frames_list.append(img_tensor)
                frames = torch.stack(frames_list, dim=0).unsqueeze(0)  # (1, T, C, H, W)
                frames = frames.to(self.device, dtype=self.dtype)
                
                # VAEç¼–ç 
                video_for_vae = [frames[0].permute(1, 0, 2, 3)]  # [(C, T, H, W)]
                latents = self.base_pipe.vae.encode(video_for_vae, device=self.device, tiled=False)
                latents = latents.permute(0, 2, 1, 3, 4).to(torch.float32)  # (1, T, C, H, W)
                
                # åº”ç”¨Temporal Module
                smoothed_list = [latents[:, 0]]
                for t in range(1, latents.shape[1]):
                    z_prev = smoothed_list[-1]
                    z_cur = latents[:, t]
                    
                    flow = self.flow_predictor(z_prev, z_cur)
                    z_fused, _, _ = self.temporal_module(
                        z_prev, z_cur,
                        s_prev=None, s_cur=None,
                        flow=flow
                    )
                    smoothed_list.append(z_fused)
                
                smoothed_latents = torch.stack(smoothed_list, dim=1)
                
                # VAEè§£ç 
                smoothed_for_decode = smoothed_latents.permute(0, 2, 1, 3, 4).to(self.dtype)
                frames_smoothed = self.base_pipe.vae.decode(
                    smoothed_for_decode,
                    device=self.device,
                    tiled=False
                )
                frames_smoothed = frames_smoothed[:, :3].permute(0, 2, 1, 3, 4)
                
                return frames_smoothed[0].cpu()  # (T, C, H, W)
    
    def generate_level(
        self,
        level_id: str,
        output_dir: Path,
        overwrite: bool = False
    ):
        """
        ç”Ÿæˆç‰¹å®šçº§åˆ«çš„æ‰€æœ‰è§†é¢‘
        
        Args:
            level_id: çº§åˆ«IDï¼ˆlevel_0, level_1, level_2, level_3ï¼‰
            output_dir: è¾“å‡ºç›®å½•
            overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„è§†é¢‘
        """
        level_config = LEVEL_CONFIGS[level_id]
        level_num = level_id.split('_')[1]  # "level_0" -> "0"
        
        print(f"\n{'='*80}")
        print(f"å¼€å§‹ç”Ÿæˆ: {level_config['name']}")
        print(f"æè¿°: {level_config['description']}")
        print(f"{'='*80}\n")
        
        # ä¿å­˜åŸºç¡€DiTçŠ¶æ€ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
        self._save_base_dit_state()
        
        # é‡ç½®Pipelineåˆ°åŸºç¡€çŠ¶æ€
        print("é‡ç½®Pipeline...")
        self._restore_base_dit_state()
        
        # åº”ç”¨çº§åˆ«é…ç½®
        if level_config.get("use_lora") and level_config.get("lora_path"):
            self._apply_lora(level_config["lora_path"])
        
        if level_config.get("dit_finetuned"):
            self._load_dit_finetuned(level_config["dit_finetuned"])
        
        if (level_config.get("use_temporal") and 
                    level_config.get("temporal_module_path") and 
                    level_config.get("flow_predictor_path")):
                    self._load_temporal_module(
                        level_config["temporal_module_path"],
                        level_config["flow_predictor_path"]
                    )
        
        # ç”Ÿæˆæ‰€æœ‰è§†é¢‘
        total_videos = len(EVALUATION_PROMPTS) * len(RANDOM_SEEDS)
        pbar = tqdm(total=total_videos, desc=f"Level {level_num}")
        
        for prompt_config in EVALUATION_PROMPTS:
            for seed in RANDOM_SEEDS:
                # æ„å»ºè¾“å‡ºæ–‡ä»¶å
                video_filename = f"video_{level_num}_{prompt_config['id']}_seed{seed}.mp4"
                video_path = output_dir / video_filename
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if video_path.exists() and not overwrite:
                    pbar.update(1)
                    pbar.set_postfix_str(f"è·³è¿‡: {video_filename}")
                    continue
                
                try:
                    # ç”Ÿæˆè§†é¢‘
                    frames = self.generate_single_video(
                        prompt=prompt_config["english"],
                        negative_prompt=prompt_config["negative"],
                        num_frames=prompt_config["num_frames"],
                        seed=seed,
                        use_temporal=level_config.get("use_temporal", False)
                    )
                    
                    # ä¿å­˜
                    save_video_frames(frames, video_path, fps=8)
                    pbar.set_postfix_str(f"å®Œæˆ: {video_filename}")
                
                except Exception as e:
                    pbar.set_postfix_str(f"é”™è¯¯: {video_filename}")
                    print(f"\nç”Ÿæˆå¤±è´¥: {video_filename}")
                    print(f"é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                
                pbar.update(1)
        
        pbar.close()
        print(f"\nâœ“ {level_config['name']} å®Œæˆ!\n")


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”Ÿæˆæ‰€æœ‰çº§åˆ«çš„è¯„ä¼°è§†é¢‘")
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./evaluation_outputs/videos",
        help="è§†é¢‘è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--levels",
        type=str,
        nargs='+',
        default=["level_0", "level_1", "level_2", "level_3"],
        choices=["level_0", "level_1", "level_2", "level_3"],
        help="è¦ç”Ÿæˆçš„çº§åˆ«"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda:0",
        help="è®¡ç®—è®¾å¤‡"
    )
    parser.add_argument(
        "--dtype",
        type=str,
        choices=["fp16", "bf16", "fp32"],
        default="fp16",
        help="ç²¾åº¦ç±»å‹"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="è¦†ç›–å·²å­˜åœ¨çš„è§†é¢‘"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # dtypeæ˜ å°„
    dtype_map = {
        "fp16": torch.float16,
        "bf16": torch.bfloat16,
        "fp32": torch.float32
    }
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = MultiLevelGenerator(
        device=args.device,
        dtype=dtype_map[args.dtype]
    )
    
    # ç”Ÿæˆæ‰€æœ‰çº§åˆ«
    for level_id in args.levels:
        generator.generate_level(
            level_id=level_id,
            output_dir=output_dir,
            overwrite=args.overwrite
        )
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰è§†é¢‘ç”Ÿæˆå®Œæˆ!")
    print(f"ä¿å­˜ä½ç½®: {output_dir}")
    print(f"æ€»è®¡è§†é¢‘æ•°: {len(list(output_dir.glob('*.mp4')))}")
    print("="*80)


if __name__ == "__main__":
    main()